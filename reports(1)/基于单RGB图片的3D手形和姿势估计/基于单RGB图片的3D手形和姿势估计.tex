\documentclass[a4paper,12pt,onecolumn,songti]{article}

\usepackage{xeCJK}
\usepackage{indentfirst} % 中文段落首行缩进
\addtolength{\topmargin}{-54pt}
\setlength{\oddsidemargin}{-0.9cm}  % 3.17cm - 1 inch
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\textwidth}{17.00cm}
\setlength{\textheight}{24.00cm}    % 24.62
\renewcommand{\baselinestretch}{1} %定义行间距
\parindent 22pt %重新定义缩进长度

\begin{document} 
	\section{文献基本信息}
	作者：Liuhao Ge1∗, Zhou Ren2, Yuncheng Li3, Zehao Xue3, Yingying Wang3, Jianfei Cai1, Junsong Yuan4;文献题目：基于单RGB图片的3D手形和姿势估计;刊物：计算机视觉与	模式识别;刊号：ISSN(1063-6919);时间：2019;格式引文：Ge, Liuhao et al. “3D Hand Shape and Pose Estimation from a Single RGB Image.” CVPR (2019).
	\section{文献讨论的问题及其主要观点}
		\paragraph{写作目的}
		基于视觉的3D手分析是非常重要的题目，因为它在虚拟现实（VR）和增强现实（AR）中有许多应用。尽管有多年的研究，但由于手的形状，姿势，手势，咬合等的多样性和复杂性，这个题目仍然有待进一步研究。最近的一些研究从单眼RGB图像中进行3D手型分析，它们主要集中在估计稀疏3D手部关节位置，忽略了密集3D手型。许多身临其境的VR和AR应用通常需要对3D手的姿势和3D手的形状进行精确估计，这促使本文提出了更具挑战性的任务：如何联合估计3D手关节位置，而且还要联合估计3D手的完整3D网格从单个RGB图像获取手表面？
		\paragraph{研究方法}
		基本思想是通过迭代优化将生成的3D手模型拟合到输入深度图像。考虑从单眼RGB图像估计3D手形，但RGB图像中缺乏明确的深度提示使得此任务难以通过迭代优化方法来解决。所以这项工作应用了经过端到端训练的深度神经网络，可以直接从单个RGB图像中恢复3D网格。具体地说，输入是一个以手为中心的单个RGB图像，通过两层式沙漏网络推断2D热图。通过使用包含八个残留层和四个最大池化层的残差网络，将估计的2D热图与图像特征图结合起来，编码为潜在特征向量。然后将编码后的潜在特征向量输入到图CNN，以推断3D手网格中的3D坐标。通过使用简化的线性图CNN，从重建的3D手网格顶点线性回归3D手关节位置。首先在异步数据集上训练网络模型，然后在真实数据集上对其进行微调。在包含3D手部网格的真实性和3D手部关节位置的综合数据集上，通过使用2D热图损失，3D网格损失和3D姿态损失，以完全监督的方式对网络进行端到端训练。
		\paragraph{主要结论}
		本文开发了基于Graph CNN的模型，从输入的RGB图像重建手表面的完整3D网格。为了训练模型，作者创建了大规模的RGB图像数据集，其中包含3D关节位置和3D手工网格的地面真相注释，并在此模型上以完全监督的方式训练模型。为了在没有3D地面真实性的现实数据集上微调模型，作者将生成的3D网格渲染到深度图，并利用观察到的深度图作为弱监督。在作者提出的新数据集和两个公共数据集进行的实验表明，该方法可以实时准确地恢复3D手部网格和3D关节位置。
	\section{文献阅读后的分析}
		以前的方法通过迭代优化将可变形手模型拟合到输入深度图，从深度图像估计3D手形和姿势。在深度学习继续发展之后，研究人员通过CNN估计深度图像中的姿势和形状参数，他们使用网格和姿势损失以端到端的方式训练CNN。但这种方法恢复的手部网格的质量受到其简单的LBS模型的限制。随着单面RGB相机的广泛应用，许多最新的著作使用深度神经网络从单个RGB图像估计3D姿势。它们主要集中在估计稀疏3D手部关节位置，忽略了密集3D手型。本文基于单RGB图像，通过图卷积神经网络（Graph CNN）重建完整的3D手表面网格来估计完整的3D手形和姿势，且实现了卓越的3D手部姿势估计精度。
\end{document} 

	